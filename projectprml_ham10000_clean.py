# -*- coding: utf-8 -*-
"""ProjectPRML_HAM10000_clean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1akNzT72VkWfg5m5oHMCpJP7bFDG6ai5w
"""

import pandas as pd
import matplotlib.pyplot as plt
import os
import glob
from tqdm import tqdm
import tensorflow as tf
import keras
import numpy as np
import cv2
import seaborn as sns
from keras.models import Sequential, Model
from keras.layers import Activation,Dense, Dropout, Flatten, Conv2D, MaxPool2D,AveragePooling2D,GlobalMaxPooling2D
from keras.layers import BatchNormalization
from keras.utils.np_utils import to_categorical
from keras import regularizers
from keras.optimizers import Adam, SGD
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report



# Encoding Target values
lesion_type_dict = {
    'nv': 'Melanocytic nevi',
    'mel': 'Melanoma',
    'bkl': 'Benign keratosis-like lesions ',
    'bcc': 'Basal cell carcinoma',
    'akiec': 'Actinic keratoses',
    'vasc': 'Vascular lesions',
    'df': 'Dermatofibroma'
}

# Reading input images
path = 'input/'

#Dictionary for Image Names
image_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in glob.glob(os.path.join(path, '*.jpg'))}

#Read Metadata csv
skin_df = pd.read_csv('HAM10000_metadata.csv')

#Create useful Columns - Images Path, Lesion Type and Lesion Categorical Code
skin_df['path'] = skin_df['image_id'].map(image_path_dict.get)
skin_df['cell_type'] = skin_df['dx'].map(lesion_type_dict.get)
skin_df['cell_type_idx'] = pd.Categorical(skin_df['cell_type']).codes

image_path_dict

skin_df

# Checking dataframe info
skin_df.info()

basic_EDA(skin_df)

summary_table(skin_df)



# Filling NaN values in 'age' column with mode of samples
skin_df['age'].fillna((skin_df['age'].mode()), inplace=True)



# Image Reading and Resizing.
IMAGE_SIZE = 32
def read_img(img_path):
  img = cv2.imread(img_path,cv2.IMREAD_COLOR)
  img = cv2.resize(img,(IMAGE_SIZE,IMAGE_SIZE))
  return img
train_img = []
for img_name in tqdm(skin_df['path'].values):
  train_img.append(read_img(img_name))

# Converting it into float arrays and normalizing it
X = np.array(train_img,np.float32)/255
print(X.shape)

# Creating Target
y = np.array(skin_df['cell_type_idx'])



"""Some classification problems do not have a balanced number of examples for each class label. As such, it is desirable to split the dataset into train and test sets in a way that preserves the same proportions of examples in each class as observed in the original dataset.
This is called a stratified train-test split.
"""

# Splitting the data
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.20,stratify=y)

# Normalization
x_train_mean = np.mean(x_train)
x_train_std = np.std(x_train)

x_train = (x_train - x_train_mean)/x_train_std
x_test = (x_test - x_train_mean)/x_train_std

# Perform one-hot encoding on the labels
y_train = to_categorical(y_train, num_classes = 7)
y_test = to_categorical(y_test, num_classes = 7)

# Splitting training into Train and Validatation sets
x_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.20,random_state=128,stratify=y_train)



# Model Parameters
input_shape = (32, 32, 3)
num_classes = 7

optimizer = Adam(learning_rate=0.001)

epochs = 100
batch_size = 15

# Callbacks
learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=5, verbose=0, factor=0.5, min_lr=0.00001)
early_stopping_monitor = EarlyStopping(patience=20,monitor='val_accuracy')

# Data Augmentation
dataaugment = ImageDataGenerator(
        rotation_range=90,  # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.1, # Randomly zoom image
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=True,  # randomly flip images
        shear_range = 10)

dataaugment.fit(x_train)





model = Sequential()
model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=input_shape))
model.add(BatchNormalization())
##############################
model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))
model.add(BatchNormalization())
model.add(AveragePooling2D(pool_size = (2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))
model.add(BatchNormalization())

model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))
model.add(BatchNormalization())
model.add(AveragePooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
##############################
model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))
model.add(BatchNormalization())
model.add(AveragePooling2D(pool_size = (2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))
model.add(BatchNormalization())

model.add(Conv2D(64, (3, 3), activation='relu',padding = 'Same'))
model.add(BatchNormalization())
model.add(AveragePooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
##############################
model.add(Flatten())

model.add(BatchNormalization())
model.add(Dense(128, activation='relu'))
model.add(Activation('relu'))
model.add(Dropout(0.25))

#Output
model.add(BatchNormalization())
model.add(Dense(num_classes, activation='softmax'))

model.summary()



model.compile(optimizer = optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])

history = model.fit(dataaugment.flow(x_train,y_train, batch_size=batch_size),
                        epochs = epochs, validation_data = (x_val,y_val),
                        verbose = 1, callbacks=[learning_rate_reduction,early_stopping_monitor])



loss = history.history['loss']
accuracy = history.history['accuracy']
val_loss = history.history['val_loss']
val_accuracy = history.history['val_accuracy']

plt.plot(range(len(loss)), loss, label="training loss")
plt.plot(range(len(val_loss)), val_loss, label="validation loss")
plt.legend(loc="upper right")
plt.figure(figsize=(6,4))
plt.show()
plt.plot(range(len(accuracy)), accuracy, label="training accuracy")
plt.plot(range(len(val_accuracy)), val_accuracy, label="validation accuracy")
plt.legend(loc="lower right")
plt.figure(figsize=(6,4))
plt.show()

model.save('prml_model.h5')

model.save_weights('prml_model_weights.h5')

final_model = keras.models.load_model('prml_model.h5')

final_model.compile(optimizer = optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])

loss_t, accuracy_t = final_model.evaluate(x_train, y_train, verbose=1)
loss_v, accuracy_v = final_model.evaluate(x_val, y_val, verbose=1)
loss, accuracy = final_model.evaluate(x_test, y_test, verbose=1)
predictions = final_model.predict(x_test)

print("Training: accuracy = %f" % (accuracy_t))
print("Validation: accuracy = %f" % (accuracy_v))
print("Test: accuracy = %f" % (accuracy))



y_test



predictions



cm = confusion_matrix(y_test.argmax(axis=1),predictions.argmax(axis=1))
cm

# PLotting Confusion Matrix
categories = ['Actinic keratoses', 'Basal cell carcinoma', 'Benign keratosis-like lesions ', 'Dermatofibroma', 'Melanocytic nevi', 'Melanoma', 'Vascular lesions']

CMatrix = pd.DataFrame( (cm) , columns=categories, index =categories)

plt.figure(figsize=(10, 5))
ax = sns.heatmap(CMatrix, annot = True, fmt = 'g' ,vmin = 0, vmax = 40,cmap = 'Reds')
ax.set_xlabel('Predicted',fontsize = 10,weight = 'bold')
ax.set_xticklabels(ax.get_xticklabels(), fontsize= 8);
ax.set_ylabel('Actual',fontsize = 10,weight = 'bold')
ax.set_yticklabels(ax.get_yticklabels(), fontsize=8)
ax.set_title('Confusion Matrix - Test Set using CNN',fontsize = 12,weight = 'bold');
plt.show()

# Calculating accuracy_score, precision_score, recall_score, f1_score using sklearn.metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

print('\nAccuracy: {:.2f}\n'.format(accuracy_score(y_test.argmax(axis=1),predictions.argmax(axis=1))))

print('Micro Precision: {:.2f}'.format(precision_score(y_test.argmax(axis=1),predictions.argmax(axis=1), average='micro')))
print('Micro Recall: {:.2f}'.format(recall_score(y_test.argmax(axis=1),predictions.argmax(axis=1), average='micro')))
print('Micro F1-score: {:.2f}\n'.format(f1_score(y_test.argmax(axis=1),predictions.argmax(axis=1), average='micro')))

print('Macro Precision: {:.2f}'.format(precision_score(y_test.argmax(axis=1),predictions.argmax(axis=1), average='macro')))
print('Macro Recall: {:.2f}'.format(recall_score(y_test.argmax(axis=1),predictions.argmax(axis=1), average='macro')))
print('Macro F1-score: {:.2f}\n'.format(f1_score(y_test.argmax(axis=1),predictions.argmax(axis=1), average='macro')))

print('Weighted Precision: {:.2f}'.format(precision_score(y_test.argmax(axis=1),predictions.argmax(axis=1), average='weighted')))
print('Weighted Recall: {:.2f}'.format(recall_score(y_test.argmax(axis=1),predictions.argmax(axis=1), average='weighted')))
print('Weighted F1-score: {:.2f}'.format(f1_score(y_test.argmax(axis=1),predictions.argmax(axis=1), average='weighted')))

print('\nClassification Report\n')
print(classification_report(y_test.argmax(axis=1),predictions.argmax(axis=1), target_names=categories))



from sklearn.metrics import roc_auc_score

macro_roc_auc_ovo = roc_auc_score(y_test, predictions, multi_class="ovo", average="macro")
weighted_roc_auc_ovo = roc_auc_score(y_test, predictions, multi_class="ovo", average="weighted")

macro_roc_auc_ovr = roc_auc_score(y_test, predictions, multi_class="ovr", average="macro")
weighted_roc_auc_ovr = roc_auc_score(y_test, predictions, multi_class="ovr", average="weighted")

print("One-vs-One ROC AUC scores:\n{:.3f} (Macro),\n{:.3f} " "(Weighted)".format(macro_roc_auc_ovo, weighted_roc_auc_ovo))
print("One-vs-Rest ROC AUC scores:\n{:.3f} (Macro),\n{:.3f} " "(Weighted)".format(macro_roc_auc_ovr, weighted_roc_auc_ovr))

import sklearn.metrics as metrics

def plot_multiclass_roc(predictions, y_test, n_classes, figsize=(10, 5)):
    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    predict_dummies = predictions
    y_test_dummies = y_test
    for i in range(n_classes):
        fpr[i], tpr[i], _ = metrics.roc_curve(y_test_dummies[:,i], predict_dummies[:,i])
        roc_auc[i] = metrics.auc(fpr[i], tpr[i])

    # roc for each class
    fig, ax = plt.subplots(figsize=figsize)
    ax.plot([2, 1], [2, 2], 'k--')
    ax.set_xlim([0.0, 1.0])
    ax.set_ylim([0.0, 1.02])
    ax.set_xlabel('False Positive Rate (FPR)',fontsize = 10,weight = 'bold')
    ax.set_ylabel('True Positive Rate (TPR)',fontsize = 10,weight = 'bold')
    ax.set_title('Receiver Operating Characteristic (ROC) curve',fontsize = 12,weight = 'bold')
    for i in range(n_classes):
        ax.plot(fpr[i], tpr[i], label='%s (AUC = %0.3f)' % (categories[i], roc_auc[i]))
    ax.legend(loc="best")
    ax.grid(alpha=.4)
    sns.despine()
    plt.show()

plot_multiclass_roc(predictions, y_test, num_classes, figsize=(8, 5))

